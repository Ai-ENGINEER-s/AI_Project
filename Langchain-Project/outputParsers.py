from langchain.schema.messages import HumanMessage , SystemMessage 
from langchain.prompts import ChatPromptTemplate
from langchain.schema import BaseOutputParser
from langchain_openai import ChatOpenAI 
from dotenv import load_dotenv 
from langchain_groq import ChatGroq 
from langchain.schema.messages import HumanMessage , SystemMessage 
from langchain.prompts import PromptTemplate


doten_dir = r"C:\Users\BARRY\Desktop\AI-WorkSpace\Langchain-Project\.env"

load_dotenv(doten_dir)
 

 # prompts are essential for guiding LLMs to generate relevant and coherent outputs . 
 # ChatPromtTemplate is verstile for creating strings prompts 

llm = ChatGroq ()

contentMessage =llm.invoke("List the seven wonders of the world ")
print(contentMessage)

for chunk in llm.stream("Where were the 2012 Olympics held ?"):
    print(chunk , end="" , flush=True)


messages = [


    SystemMessage(content="You are Mickael Jordan ."),
    HumanMessage(content="Wich shoe manufacturer are you associated with ? ")
]

response = llm.invoke(messages)
print(response)

# prompts models and their use cases 

#  1 Chat promptemplate 


promptMessage = PromptTemplate.from_template(
    "Tell me {adjective} joke about {content}"
)

filled_prompt= promptMessage.format(adjective ="Funny", content="Robot")
print(filled_prompt)


# 2 ChatPromptTemplate 

chtTmp= ChatPromptTemplate.from_messages(
    [   ("human", "Hello, how are you?"),
            ("ai", "I'm doing well, thanks!"),
            ("human", "That's good to hear."),]
)

responses = chtTmp.format_messages(name ="Bob", userInput="what is your Name ?")

for message in responses : 
    print(message)




#  Output Parsers  what that means ? 

# OutputParsers are a way for users to structure the output of the responses generated by the LLMs . In other , this allows the users to shape in  his own way the output of the llms . 


# 1 PydanticOutputParser 

from langchain.output_parsers import PydanticOutputParser
from langchain.pydantic_v1 import BaseModel , Field , validator
from typing import List 


class FirstNameLastName(BaseModel):
    firstName: str = Field("What is your first name ?")
    lastNmae : str = Field("What is your lastName ?")


parser = PydanticOutputParser(pydantic_object=FirstNameLastName)

chain = llm | parser 
prompt = PromptTemplate(
    template="Answer the user query.\n{format_instructions}\n{query}\n", 
    input_variables=["query"],
    partial_variables={"format_instructions":parser.get_format_instructions()}
)


query = "what is last name ?"
chain = prompt | llm 
output = chain.invoke({"query": query})
parsed_result = parser.invoke(output)

print(parsed_result)


# 2 SimpleJsonOutputParser 

#create a Json PROMPT 

json_prompt = PromptTemplate.from_template(
    "Return a Json Object with `birthday` `birthplace` key that answers the following question : {question}"
)

json_chain = json_prompt | llm

# 3 CommaSepartedListOutputParser 

# it's handy when you want to extract comma-separated lists from model responses 

from langchain.output_parsers import CommaSeparatedListOutputParser

ouputparser = CommaSeparatedListOutputParser()

format_instructions = ouputparser.get_format_instructions()

prompt12 = PromptTemplate(
    template="List five {subject}.\n{format_instructions}", 
    input_variables=["subject"],
    partial_variables={"format_instructions":format_instructions}
)
